{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IlyaZutler/Project_2-Trucks/blob/main/DM%20_%20Project_2_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dynamic mitochondria Project - Heavy Machinery Auction Price Estimator\n",
        "\n",
        "> https://www.kaggle.com/t/9baafb8850d74e4499c7b1ba97d6f115\n",
        "\n",
        "### Timeline\n",
        "- **Start Date:** [Start Date]\n",
        "- **End Date:** 14/07/2024 (11 days to go)\n",
        "\n",
        "## Getting Started\n",
        "\n",
        "### 5. RandomForestRegressor Model\n",
        "\n",
        "### 6. Model Improvement\n",
        "\n",
        "- Handle missing values and categorical variables more effectively.\n",
        "- Use feature importances to identify key features.\n",
        "- Perform feature engineering to create new informative features.\n",
        "- Tune hyperparameters using grid search or other techniques.\n",
        "- Monitor for overfitting by comparing training and testing performance.\n",
        "\n",
        "### 7. Final Submission\n",
        "\n",
        "Generate predictions for the validation set:\n",
        "\n",
        "```python\n",
        "valid = pd.read_csv('valid.csv')\n",
        "X_valid = valid.drop(columns=['SalesID'])\n",
        "y_valid_pred = model.predict(X_valid)\n",
        "\n",
        "# Create a submission file\n",
        "submission = pd.DataFrame({'SalesID': valid['SalesID'], 'SalePrice': y_valid_pred})\n",
        "submission.to_csv('final_submission.csv', index=False)\n",
        "```\n",
        "\n",
        "## Practical Data Science Guidelines\n",
        "\n",
        "- **Efficient Workflows:** Use a random subset of 20,000 rows for initial experiments. Use the full dataset for the final submission.\n",
        "- **Iterative Approach:** Start with a basic model and iteratively improve it by trying small ideas.\n",
        "- **Feature Engineering:** Transform and combine existing features creatively.\n",
        "- **Documentation:** Keep track of your experiments and results. Document what works and what doesn't.\n",
        "\n",
        "## Collaboration and Presentation\n",
        "\n",
        "- **Collaboration:** Discuss your work openly within your team or with other teams. Sharing insights and learning from each other is encouraged.\n",
        "- **Presentation:** Present your methodology, results, and the techniques that helped the most. Document your journey and the steps you took to achieve your results\n",
        "\n"
      ],
      "metadata": {
        "id": "nipVcj4V_QDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "pd.set_option('display.max_rows', 200)\n",
        "pd.set_option('display.max_columns', 200)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error"
      ],
      "metadata": {
        "id": "ddCt_SZwAXG6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_from_gdrive(url, filename):\n",
        "    # Extract the file ID from the URL\n",
        "    file_id = url.split('/')[-2]\n",
        "    download_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "\n",
        "    # Download the file\n",
        "    if Path(filename).exists():\n",
        "        print(f\"File '{filename}' already exists. Skipping download.\")\n",
        "    else:\n",
        "        gdown.download(download_url, filename, quiet=False)\n",
        "        print(f\"File downloaded as: {filename}\")\n",
        "\n",
        "train = 'https://drive.google.com/file/d/1guqSpDv1Q7ZZjSbXMYGbrTvGns0VCyU5/view?usp=drive_link'\n",
        "valid = 'https://drive.google.com/file/d/1j7x8xhMimKbvW62D-XeDfuRyj9ia636q/view?usp=drive_link'\n",
        "# Example usage\n",
        "\n",
        "download_from_gdrive(train, 'train.csv')\n",
        "download_from_gdrive(valid, 'valid.csv')\n",
        "\n",
        "df = pd.read_csv('train.csv')\n",
        "df_valid = pd.read_csv('valid.csv')\n",
        "df.head().T"
      ],
      "metadata": {
        "id": "2jdlNVVI8s_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploratory Data Analysis (EDA)"
      ],
      "metadata": {
        "id": "dIQ1BlV7wZBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#df.fiProductClassDesc.value_counts()"
      ],
      "metadata": {
        "id": "30KP0HEQbhrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df.info()"
      ],
      "metadata": {
        "id": "pfswzVUcRrUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df.isnull().sum()"
      ],
      "metadata": {
        "id": "lEziG3fNwb-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df.describe()"
      ],
      "metadata": {
        "id": "vLqhFjn2R21j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sns.histplot(data=df, x='SalePrice', bins=20)"
      ],
      "metadata": {
        "id": "TCdKm_ok94x5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to see value_counts for all categorical columns, but some realy categorical columns has numerical type like ModelID\n",
        "categorical_cols = df.select_dtypes(exclude='number').columns\n",
        "for col in categorical_cols:\n",
        "  print(f\"Value counts for column '{col}':\")\n",
        "  print(df[col].value_counts())\n",
        "  print(f\"NaN values:{df[col].isnull().sum()}\")\n",
        "  print()"
      ],
      "metadata": {
        "id": "Zp2RHhayXZWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Data Preprocessing"
      ],
      "metadata": {
        "id": "PhCFYLEqQimv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_transmission(X):\n",
        "    X['Transmission'] = X['Transmission'].replace('AutoShift', 'Autoshift')\n",
        "    return X"
      ],
      "metadata": {
        "id": "yaRgPnHY5hxN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "source": [
        "def fix_year(X):\n",
        "    X['saledate'] = pd.to_datetime(X['saledate'])\n",
        "    X['saleYear'] = X['saledate'].dt.year\n",
        "    X['saleMonth'] = X['saledate'].dt.month\n",
        "    X = X.drop('saledate', axis=1)\n",
        "    return X"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "tD7dI7Yy7RYF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_first_word(X):\n",
        "    X['fiProductClassDesc_first_word'] = X['fiProductClassDesc'].apply(lambda x: x.split()[0])\n",
        "    return X"
      ],
      "metadata": {
        "id": "ENaDPhA60coK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OrdinalEncoder for column 'ProductSize':\n",
        "# df['ProductSize_3'] = df['ProductSize'].map({'Compact': 1, 'Mini': 2, 'High': 3, 'Small': 4, 'Medium': 5, 'High': 6, 'Large / Medium': 7, 'Large': 8})\n",
        "# df['ProductSize_3'] = df['ProductSize_3'].fillna(df['ProductSize_3'].mean())\n",
        "# df['ProductSize_3'].value_counts(), df['ProductSize_3'].isnull().sum()"
      ],
      "metadata": {
        "id": "xbYKmRxCEJLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# processing of categorical ordinal features - using libruary Sklern OrdinalEncoder\n",
        "# encoder = OrdinalEncoder(categories=[['Low', 'Medium', 'High']], handle_unknown='use_encoded_value', unknown_value= -1) # Handle unknown values\n",
        "# df['UsageBand_3'] = encoder.fit_transform(df[['UsageBand']])\n",
        "# mean_real = df['UsageBand_3'][df.UsageBand_3.isin([0,1,2])].mean()\n",
        "# df['UsageBand_3'] = df['UsageBand_3'].replace(-1, mean_real)\n",
        "# # Fix: Fill missing values in the 'UsageBand_3' column and keep it in the DataFrame\n",
        "# df['UsageBand_3'] = df['UsageBand_3'].fillna(mean_real)\n",
        "# df['UsageBand_3'].value_counts(), df['UsageBand_3'].isnull().sum()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "t0dhntPlTicF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# 'Undercarriage_Pad_Width' - remove ' inch' from data - to numeric\n",
        "# df['Undercarriage_Pad_Width_3'] = df['Undercarriage_Pad_Width'].str.replace(' inch', '').replace('None or Unspecified', -1)\n",
        "# df['Undercarriage_Pad_Width_3'] = pd.to_numeric(df['Undercarriage_Pad_Width_3'])\n",
        "# mean_real_2 = df['Undercarriage_Pad_Width_3'][df.Undercarriage_Pad_Width_3 != -1].mean()\n",
        "# df['Undercarriage_Pad_Width_3'] = df['Undercarriage_Pad_Width_3'].replace(-1, mean_real_2)\n",
        "# df['Undercarriage_Pad_Width_3'] = df['Undercarriage_Pad_Width_3'].fillna(mean_real_2)\n",
        "# df['Undercarriage_Pad_Width_3'].value_counts(),df['Undercarriage_Pad_Width_3'].isnull().sum()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "7-jMMuVt4E4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 'Stick_Length' - remove \" from data - to numeric\n",
        "# df['Stick_Length_3'] = df['Stick_Length'].str.replace(\"' \", '.').str.replace('\"', '').replace('None or Unspecified', -1)\n",
        "# df['Stick_Length_3'] = pd.to_numeric(df['Stick_Length_3'])\n",
        "# mean_real_3 = df['Stick_Length_3'][df.Stick_Length_3.isna() | df.Stick_Length_3 != -1 ].mean()\n",
        "# print(mean_real_3)\n",
        "# df['Stick_Length_3'] = df['Stick_Length_3'].replace(-1, mean_real_3)\n",
        "# df['Stick_Length_3'] = df['Stick_Length_3'].fillna(mean_real_3)\n",
        "# df['Stick_Length_3'].value_counts(), df['Stick_Length_3'].isnull().sum()"
      ],
      "metadata": {
        "id": "hOf7ElvKy73D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df[df['YearMade'] == 1000].head(20) # i have not idias what to do with year 1000"
      ],
      "metadata": {
        "id": "oEEe-qSaHyHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mean / Target coding"
      ],
      "metadata": {
        "id": "zmFxzf7MFOTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MeanTargetEncode(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self.target_mean_dict = None\n",
        "        self.target_nan_mean_dict = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X = X.copy()\n",
        "        y = pd.DataFrame(y)\n",
        "        X = pd.concat([X, y], axis=1)\n",
        "        X = X.rename(columns={0: 'SalePrice'})\n",
        "\n",
        "        self.target_mean_dict = dict()\n",
        "        self.target_nan_mean_dict = dict()\n",
        "\n",
        "        for col in X.select_dtypes(exclude='number').columns:\n",
        "            self.target_mean_dict[col] = X.groupby(col)['SalePrice'].mean().to_dict()\n",
        "            self.target_nan_mean_dict[col] = X[X[col].isna()]['SalePrice'].mean()\n",
        "\n",
        "        X = X.drop(columns=['SalePrice'])\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "\n",
        "        for col in X.select_dtypes(exclude='number').columns:\n",
        "            X[col + '_2'] = X[col].map(self.target_mean_dict[col]).fillna(self.target_nan_mean_dict[col])\n",
        "            X[col + '_2'] = X[col + '_2'].astype(float)\n",
        "\n",
        "        return X\n"
      ],
      "metadata": {
        "id": "XNSZH0gjMmB1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datas for model"
      ],
      "metadata": {
        "id": "GqOepCDgRE4N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def drop_columns(X, col):\n",
        "    X = X.drop(columns = col)\n",
        "    return X"
      ],
      "metadata": {
        "id": "W5dzpndJ0uTj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(X):\n",
        "    X = X.select_dtypes('number')  # drop all categorical variables\n",
        "    return X"
      ],
      "metadata": {
        "id": "z2l_DyVw03Xr"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_2 = df.sample(150000)\n",
        "\n",
        "y = df_2['SalePrice']\n",
        "X = df_2.drop(columns=['SalePrice'])\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('transmission_fixer', FunctionTransformer(fix_transmission)),\n",
        "    ('year_fixer', FunctionTransformer(fix_year)),\n",
        "    ('first_word_fixer', FunctionTransformer(fix_first_word)),\n",
        "    ('mean_target_encode', MeanTargetEncode()),\n",
        "    ('drop', FunctionTransformer(drop_columns, kw_args={'col': ['MachineID', 'ModelID','SalesID']})),\n",
        "    ('prepare_dats', FunctionTransformer(prepare_data)),\n",
        "    ('fillna', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', MinMaxScaler()),\n",
        "    ('model', RandomForestRegressor(n_jobs=-1,\n",
        "                              n_estimators = 200,\n",
        "                              #max_depth = 12,\n",
        "                              min_impurity_decrease = 1000,\n",
        "                              random_state=42\n",
        "                              ))\n",
        "])\n",
        "\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "y_train_pred = pipeline.predict(X_train)\n",
        "\n",
        "y_test_pred = pipeline.predict(X_test)\n",
        "\n",
        "print(f'Train RMSE:', np.sqrt(mean_squared_error(y_train, y_train_pred)))\n",
        "print(f'Test RMSE:', np.sqrt(mean_squared_error(y_test, y_test_pred)))\n",
        "print(f'R²:' , r2_score(y_test, y_test_pred))\n",
        "print(f'Train MAE:', mean_absolute_error(y_train, y_train_pred))\n",
        "print(f'Test MAE:', mean_absolute_error(y_test, y_test_pred))\n"
      ],
      "metadata": {
        "id": "jBhQYisbBXcJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca9442ee-b9f0-41e7-8c7c-8403226ae136"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train RMSE: 3601.1786361075306\n",
            "Test RMSE: 7836.2937888276765\n",
            "R²: 0.8852581841956384\n",
            "Train MAE: 2690.375799904813\n",
            "Test MAE: 4919.277895616473\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "regressor = pipeline.named_steps['model']\n",
        "feature_importances = regressor.feature_importances_\n",
        "feature_names = regressor.feature_names_in_\n",
        "\n",
        "# Создаем DataFrame с важностями признаков и сортируем их\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': feature_importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(feature_importance_df)"
      ],
      "metadata": {
        "id": "jcMpXIs0PO_m",
        "outputId": "082beb65-bac3-4fc0-9c14-5e2faf04b319",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'RandomForestRegressor' object has no attribute 'feature_names_in_'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-79-aab6a92ee31d>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mregressor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfeature_importances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfeature_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_names_in_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Создаем DataFrame с важностями признаков и сортируем их\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'RandomForestRegressor' object has no attribute 'feature_names_in_'"
          ]
        }
      ]
    }
  ]
}